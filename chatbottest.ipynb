{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### store the embeddings and faiss vectors with smaller chunks and then retrieve those documents,\n",
    "####### run similarity check and then check the results\n",
    "import logging\n",
    "import pathlib\n",
    "from langchain_core.documents import Document\n",
    "from langchain_community.vectorstores.docarray import DocArrayInMemorySearch\n",
    "from langchain_community.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_core.retrievers import BaseRetriever\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.document_loaders import (\n",
    "  PyPDFLoader, TextLoader,\n",
    "  UnstructuredWordDocumentLoader,\n",
    "  UnstructuredEPubLoader\n",
    ")\n",
    "from langchain_community.vectorstores import docarray\n",
    "from langchain.vectorstores.faiss import FAISS\n",
    "from langchain.chains.conversational_retrieval.base import ConversationalRetrievalChain\n",
    "#from langchain_openai import ChatOpenAI\n",
    "from langchain.chains.base import Chain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "import streamlit as st\n",
    "from streamlit.external.langchain import StreamlitCallbackHandler\n",
    "from ctransformers import AutoModelForCausalLM\n",
    "from langchain.llms import CTransformers\n",
    "#from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.llms import OpenAI\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "from transformers import LlamaForCausalLM, LlamaTokenizer\n",
    "\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain.llms import HuggingFacePipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_126624\\2938737422.py:1: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings_hf = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n"
     ]
    }
   ],
   "source": [
    "embeddings_hf = HuggingFaceEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "load_path = 'vector_store/retr_mistral_clean1'\n",
    "vectordb = FAISS.load_local(folder_path = load_path, embeddings = embeddings_hf, allow_dangerous_deserialization = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login()  # This will prompt you to enter your API token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the disk and cpu.\n",
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is HER-2/neu? (IgM)\n",
      "IgM is a type of antibody that plays a key role in the immune system, particularly in fighting off infections and diseases. One of its main jobs is to target cells that have a specific protein called HER-2/neu, which is also known as human epidermal growth factor receptor 2.\n",
      "\n",
      "Now, let's think about where this HER-2/neu protein comes from. It's actually encoded by a gene called the HER2 gene on the chromosome 17. This gene is part of the human genome, and it's crucial for producing proteins that can help the body fight off cancer.\n",
      "\n",
      "When it comes to HER-2/neu, it's not just about being a protein; it's actually a receptor on the surface of certain cells. These cells, especially in the context of breast and other cancers, can have an overabundance of this receptor.\n",
      "\n",
      "The connection to HER-2/neu isn't just theoretical; there's a specific antigen on these cancer cells that triggers the immune system to produce antibodies, like IgM, to fight off the cancer. This is a part of the immune system's strategy to recognize and attack cancer cells.\n",
      "\n",
      "So, putting it all together, HER-2/neu is indeed a protein, and it's encoded by the HER2 gene on chromosome 17. It's a receptor on the surface of cancer cells, particularly in the context of breast and other cancers. And, of course, the body produces antibodies like IgM in response to this antigen, specifically targeting those cancer cells.\n",
      "\n",
      "It's all about understanding the protein's role, its origin, and how the body's immune system responds to it. This makes sense when you think about the immune system's efforts to combat cancer, where HER-2/neu is a significant target.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "\n",
    "# Model name\n",
    "model_name = \"ContactDoctor/Bio-Medical-Llama-3-2-1B-CoT-012025\"\n",
    "\n",
    "# Load model & tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "\n",
    "# Ensure model runs on GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#model.to(device)\n",
    "\n",
    "# Function to generate responses\n",
    "def generate_text(prompt):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)  # Move input to GPU\n",
    "    output = model.generate(**inputs, max_length=512)\n",
    "    return tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "# Test\n",
    "response = generate_text(\"What is HER-2/neu?\")\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_126624\\4078974433.py:5: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n"
     ]
    }
   ],
   "source": [
    "# ✅ Wrap model in Hugging Face pipeline\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=256)\n",
    "\n",
    "# ✅ Use HuggingFacePipeline as LLM in LangChain\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_126624\\1428512797.py:5: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table 2 describes the relationship between the amplification of HER-2/neu and the disease parameters in 103 breast tumors. It shows that patients with amplification of HER-2/neu tend to have a higher recurrence rate and shorter overall survival compared to those without amplification. This is in contrast to the association with EGFR, where there is no significant correlation with disease-free survival.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# Store conversation history\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "retriever1 = vectordb.as_retriever(search_kwargs={\"k\":10})\n",
    "# Create Retrieval Chain\n",
    "qa_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,  # Use the LLaMA 3 model as LLM\n",
    "    retriever=retriever1,\n",
    "    memory=memory,\n",
    "    return_source_documents=False\n",
    "  \n",
    ")\n",
    "\n",
    "# Test query\n",
    "query = \"what does table 2 describe?\"\n",
    "response = qa_chain.invoke({\"question\": query})\n",
    "\n",
    "#print(response[\"answer\"])\n",
    "\n",
    "clean_response = response[\"answer\"].split(\"Helpful Answer:\")[-1].strip()\n",
    "print(clean_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'publication1.pdf'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_filepath = 'data'\n",
    "os.listdir(temp_filepath)[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
